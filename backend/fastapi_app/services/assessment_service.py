from typing import Dict, List, Any
from tempfile import NamedTemporaryFile
from fastapi import UploadFile, HTTPException, Request
from fastapi_app.schemas.test_schemas import PreferenceData, FinalAssessmentSubmission, QuizQuestion 
import os
import json
import logging
import google.generativeai as genai
from google.genai import types as g_types
from starlette.concurrency import run_in_threadpool
import base64, mimetypes
from fastapi_app.database import admin_supabase # D√πng db_client t·ª´ database.py


logger = logging.getLogger(__name__)

# T·∫≠n d·ª•ng client ƒë√£ kh·ªüi t·∫°o ·ªü ph·∫°m vi global t·ª´ test_service
try:
    from .test_service import client, GEMINI_MODEL 
except ImportError:
    client = None
    GEMINI_MODEL = "gemini-2.0-flash"


# --- H√ÄM 1: STT V√Ä PH√ÇN T√çCH TRANSCRIPT (Gi·ªØ nguy√™n) ---

async def run_stt_and_analysis_sync(audio_path: str, client):
    def _sync_call():
        with open(audio_path, "rb") as f:
            audio_data = f.read()

        return client.models.generate_content(
            model="models/gemini-2.0-flash",
            contents=[
                {
                    "role": "user",
                    "parts": [
                        {"text": "Please transcribe this audio."},
                        {
                            "inline_data": {
                                "mime_type": "audio/mpeg",
                                "data": base64.b64encode(audio_data).decode("utf-8"), # S·ª≠a base64 encoding
                            }
                        }
                    ]
                }
            ]
        )

    response = await run_in_threadpool(_sync_call)
    transcript = response.text
    word_count = len(transcript.split()) 

    return {
        "transcript": transcript,
        "word_count": word_count
    }
async def analyze_transcript_with_gemini(transcript: str) -> str:
    """G·ªçi Gemini ƒë·ªÉ ƒë√°nh gi√° ng·ªØ ph√°p/t·ª´ v·ª±ng trong transcript c·ªßa ng∆∞·ªùi d√πng."""
    analysis_prompt = f"Ph√¢n t√≠ch vƒÉn b·∫£n: '{transcript}' v·ªÅ l·ªói ng·ªØ ph√°p, ch·∫•t l∆∞·ª£ng t·ª´ v·ª±ng, v√† ƒë∆∞a ra 2 g·ª£i √Ω c·∫£i thi·ªán."
    try:
        analysis_response = await run_in_threadpool(
            client.models.generate_content,
            model=GEMINI_MODEL,
            contents=[analysis_prompt]
        )
        return analysis_response.text.strip()
    except Exception as e:
        logger.error(f"L·ªói ph√¢n t√≠ch Transcript LLM: {e}")
        return "L·ªói ph√¢n t√≠ch. Vui l√≤ng th·ª≠ l·∫°i b√†i n√≥i."


# --- H√ÄM 2: CH·∫§M ƒêI·ªÇM TR·∫ÆC NGHI·ªÜM TH·ª∞C T·∫æ ---

def calculate_mcq_score(
    mcq_answers: Dict[str, str] | None, 
    quiz_questions: List[QuizQuestion] 
) -> Dict[str, Any]: 
    """H√†m t√≠nh ƒëi·ªÉm tr·∫Øc nghi·ªám (MCQ) b·∫±ng c√°ch so s√°nh v·ªõi c√¢u tr·∫£ l·ªùi LLM."""
    
    if mcq_answers is None:
        mcq_answers = {}
        
    total_answered = len(mcq_answers)
    correct_count = 0
    topic_results = {} 
    
    correct_data_map = {}
    for q in quiz_questions:
        if q.question_type != 'speaking_prompt':
            correct_data_map[str(q.id)] = {
                'correct_key': q.correct_answer_key, 
                'topic': q.question_type 
            }
            
    for q_id, user_key in mcq_answers.items():
        quiz_data = correct_data_map.get(q_id)
        
        if quiz_data:
            topic = quiz_data['topic']
            correct_answer = quiz_data['correct_key']
            
            if topic not in topic_results:
                topic_results[topic] = [0, 0] 
            
            topic_results[topic][1] += 1
            
            if correct_answer == user_key:
                correct_count += 1
                topic_results[topic][0] += 1
            
    weak_topics = []
    WEAK_THRESHOLD = 0.60 
    
    for topic, (correct, total) in topic_results.items():
        if total > 0 and (correct / total) < WEAK_THRESHOLD:
            weak_topics.append(f"{topic} (ƒê√∫ng: {correct}/{total})")

    if not weak_topics and total_answered > 0:
        weak_topics.append("Kh√¥ng ph√°t hi·ªán ƒëi·ªÉm y·∫øu l·ªõn ·ªü ph·∫ßn tr·∫Øc nghi·ªám.")

    score_percent = (correct_count / total_answered) * 100 if total_answered > 0 else 0
    
    return {
        "score_percent": score_percent,
        "correct_count": correct_count,
        "total_questions": total_answered,
        "weak_topics": weak_topics,
        "estimated_level": "Intermediate (B1)" if score_percent >= 60 else "Pre-Intermediate (A2)",
    }

# -----------------------------------------------------------------

async def analyze_and_generate_roadmap(
    payload_data: FinalAssessmentSubmission,
    audio_files: Dict[str, UploadFile]
) -> Dict[str, Any]:
    if client is None:
        raise HTTPException(status_code=500, detail="Gemini Client kh√¥ng kh·∫£ d·ª•ng.")

    # Ch·ªâ log audio files
    logger.info(f"üìå FILE MAP NH·∫¨N T·ª™ FRONTEND: {list(audio_files.keys())}")

    # --- 1. PH√ÇN T√çCH MCQ ---
    mcq_analysis = calculate_mcq_score(payload_data.mcq_answers, payload_data.quiz_questions)
    diagnostic_summary = mcq_analysis 
    # --- 2. X·ª¨ L√ù SPEAKING ---
    full_speaking_analysis = []

    # T·∫°o map t·ª´ file_key (d√π frontend g·ª≠i g√¨) v·ªÅ t√™n file th·ª±c t·∫ø trong form
    file_key_to_form_key = {}
    for form_key in audio_files.keys():
        # form_key c√≥ th·ªÉ l√† 'audio_file_21' ho·∫∑c 'audio_file_21[]' ho·∫∑c 'audio_21' t√πy frontend
        logger.debug(f"Processing form_key for mapping: {form_key}")
        if isinstance(form_key, str):
            key = form_key
            # direct numeric extraction
            if key.startswith("audio_file_"):
                num = key.replace("audio_file_", "")
                file_key_to_form_key[num] = form_key
                file_key_to_form_key[int(num)] = form_key
            else:
                # try extract last numeric part
                import re
                m = re.search(r"(\d+)", key)
                if m:
                    num = m.group(1)
                    file_key_to_form_key[num] = form_key
                    file_key_to_form_key[int(num)] = form_key
                # also map the raw key itself
                file_key_to_form_key[key] = form_key

    logger.info(f"[service] File key mapping (after scan): {file_key_to_form_key}")

    # Log speaking_data coming in payload for debug
    try:
        logger.info(f"[service] speaking_data payload: {payload_data.speaking_data}")
    except Exception:
        logger.exception("Kh√¥ng th·ªÉ log speaking_data")

    for speaking_data_item in payload_data.speaking_data:
        raw_key = speaking_data_item.file_key
        logger.info(f"[service] Raw file_key t·ª´ frontend: {raw_key} (type: {type(raw_key)})")

        # Chu·∫©n h√≥a key: th·ª≠ t·∫•t c·∫£ c√°c kh·∫£ nƒÉng
        possible_keys = []
        try:
            possible_keys = [
                str(raw_key).strip(),
                str(raw_key).strip().lstrip("Qq"),
                str(raw_key).strip().replace("question_", ""),
                f"audio_file_{str(raw_key).strip()}",
                f"audio_{str(raw_key).strip()}",
            ]
        except Exception:
            possible_keys = [str(raw_key)]

        # N·∫øu raw_key l√† s·ªë d·∫°ng int
        if isinstance(raw_key, (int, float)):
            possible_keys.append(str(int(raw_key)))

        # Deduplicate
        seen = set()
        possible_keys = [k for k in possible_keys if not (k in seen or seen.add(k))]

        logger.debug(f"[service] possible_keys to try for raw_key {raw_key}: {possible_keys}")

        matched_form_key = None
        for k in possible_keys:
            # 1) direct in mapping dict
            if k in file_key_to_form_key:
                matched_form_key = file_key_to_form_key[k]
                logger.info(f"[service] matched via file_key_to_form_key: {k} -> {matched_form_key}")
                break
            # 2) direct form key present
            if k in audio_files:
                matched_form_key = k
                logger.info(f"[service] matched direct form key: {k}")
                break
            # 3) try with audio_file_ prefix
            prefix = f"audio_file_{k}"
            if prefix in audio_files:
                matched_form_key = prefix
                logger.info(f"[service] matched with prefix: {prefix}")
                break

        audio_file = audio_files.get(matched_form_key) if matched_form_key else None

        if not audio_file:
            logger.warning(f"Kh√¥ng t√¨m th·∫•y audio cho file_key={raw_key} (ƒë√£ th·ª≠: {possible_keys})")
            logger.warning(f"C√°c key c√≥ s·∫µn: {list(audio_files.keys())}")
            # fallback: n·∫øu ch·ªâ c√≥ 1 file, gi·∫£ s·ª≠ map v√†o ƒë√≥ (ch·ªâ ƒë·ªÉ debug, c√≥ th·ªÉ lo·∫°i b·ªè s·∫£n xu·∫•t)
            if len(audio_files) == 1:
                only_key = list(audio_files.keys())[0]
                logger.warning(f"[service] Fallback: ch·ªâ c√≥ 1 file upload, d√πng {only_key}")
                audio_file = audio_files.get(only_key)
            else:
                continue
        else:
            logger.info(f"ƒê√É T√åM TH·∫§Y audio cho Q{raw_key}: {matched_form_key} -> filename: {getattr(audio_file,'filename',None)}")

        # --- Ki·ªÉm tra nhanh n·ªôi dung file (size) tr∆∞·ªõc khi ghi temp ---
        try:
            # Kh√¥ng ƒë·ªçc to√†n b·ªô n·∫øu l·ªõn ‚Äî nh∆∞ng UploadFile h·ªó tr·ª£ .file.tell() n·∫øu c·∫ßn
            # ·ªû ƒë√¢y ch·ªâ ƒë·ªÉ log size approximate n·∫øu c√≥ attribute .file
            try:
                file_obj = audio_file.file
                file_obj.seek(0, 2)
                size = file_obj.tell()
                file_obj.seek(0)
            except Exception:
                size = None
            logger.info(f"[service] File info - key: {matched_form_key}, filename: {getattr(audio_file,'filename',None)}, size_bytes: {size}")
        except Exception:
            logger.exception("Kh√¥ng th·ªÉ l·∫•y file size")

        # --- T·ª´ ƒë√¢y gi·ªØ nguy√™n x·ª≠ l√Ω file ---
        tmp_path = None
        try:
            file_content = await audio_file.read()
            suffix = os.path.splitext(audio_file.filename)[1] or ".mp3"

            with NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
                tmp_path = tmp.name
                await run_in_threadpool(tmp.write, file_content)

            logger.info(f"[service] Vi·∫øt t·∫°m file: {tmp_path}")

            # STT
            stt_result = await run_stt_and_analysis_sync(tmp_path, client)

            # Gemini Grammar
            llm_comment = await analyze_transcript_with_gemini(stt_result['transcript'], client)

            full_speaking_analysis.append({
                "question_id": raw_key,
                "transcript": stt_result['transcript'],
                "word_count": stt_result['word_count'],
                "latency_s": speaking_data_item.latency_ms / 1000,
                "llm_grammar_comment": llm_comment,
            })

        except Exception as e:
            logger.exception(f"L·ªói x·ª≠ l√Ω audio cho Q{raw_key}: {e}")

        finally:
            if tmp_path and os.path.exists(tmp_path):
                try:
                    os.unlink(tmp_path)
                except:
                    pass
    # --- 3. X√ÇY D·ª∞NG PROMPT CHO GEMINI v√† t·∫°o roadmap ---
    prefs = payload_data.preferences
    prefs_dict = prefs.model_dump()

    weak_points_list = list(mcq_analysis.get('weak_topics', []))
    has_speaking = len(full_speaking_analysis) > 0
    if has_speaking and full_speaking_analysis[0]['latency_s'] > 1.5:
        weak_points_list.append("Ph·∫£n x·∫° ch·∫≠m (Latency > 1.5s)")

    speaking_transcript = full_speaking_analysis[0]['transcript'] if has_speaking else "Kh√¥ng c√≥ d·ªØ li·ªáu n√≥i."

    roadmap_prompt = f"""
    B·∫°n l√† chuy√™n gia thi·∫øt k·∫ø l·ªô tr√¨nh h·ªçc ti·∫øng Anh giao ti·∫øp.

    Tr∆∞·ªõc ti√™n, h√£y **nh·∫≠n x√©t t·ªïng quan v·ªÅ ng∆∞·ªùi h·ªçc d·ª±a tr√™n c√°c th√¥ng tin sau**:
    - K·∫øt qu·∫£ b√†i test: {mcq_analysis}
    - ƒêi·ªÉm y·∫øu hi·ªán t·∫°i: {", ".join(weak_points_list)}
    - Transcript n√≥i m·∫´u: {speaking_transcript}
    - Cam k·∫øt h·ªçc m·ªói ng√†y: {prefs_dict['daily_commitment']}
    - M·ª•c ti√™u: {prefs_dict['communication_goal']}
    - Th·ªùi gian mong mu·ªën: {prefs_dict['target_duration']}

    Nh·∫≠n x√©t c·∫ßn n√™u r√µ:
    - Tr√¨nh ƒë·ªô hi·ªán t·∫°i c·ªßa ng∆∞·ªùi h·ªçc
    - ƒêi·ªÉm m·∫°nh / ƒëi·ªÉm y·∫øu n·ªïi b·∫≠t
    - Kh·∫£ nƒÉng ho√†n th√†nh m·ª•c ti√™u d·ª±a tr√™n th·ªùi gian cam k·∫øt
    - Khuy·∫øn ngh·ªã t·ªïng quan tr∆∞·ªõc khi ƒëi v√†o l·ªô tr√¨nh

    Sau ƒë√≥, d·ª±a v√†o c√°c th√¥ng tin tr√™n, h√£y t·∫°o **l·ªô tr√¨nh h·ªçc c√° nh√¢n h√≥a**:
    - S·ªë giai ƒëo·∫°n: linh ho·∫°t, t√πy thu·ªôc v√†o k·∫øt qu·∫£ test v√† th·ªùi gian mong mu·ªën c·ªßa ng∆∞·ªùi h·ªçc
    - M·ªói giai ƒëo·∫°n g·ªìm: t√™n giai ƒëo·∫°n, th·ªùi l∆∞·ª£ng, tr·ªçng t√¢m h·ªçc, daily plan, expected outcomes, milestone
    - N·ªôi dung l·ªô tr√¨nh ph√π h·ª£p v·ªõi th·ªùi gian cam k·∫øt h√†ng ng√†y, m·ª•c ti√™u v√† ƒëi·ªÉm y·∫øu c·ªßa ng∆∞·ªùi h·ªçc
    - ƒê·∫£m b·∫£o l·ªô tr√¨nh v·ª´a th·ª±c t·∫ø v·ª´a hi·ªáu qu·∫£, tr√°nh qu√° t·∫£i

    TR·∫¢ V·ªÄ **CH·ªà M·ªòT JSON DUY NH·∫§T** v·ªõi c·∫•u tr√∫c:

    {{
    "user_summary": "Nh·∫≠n x√©t t·ªïng quan v·ªÅ ng∆∞·ªùi h·ªçc d·ª±a tr√™n k·∫øt qu·∫£ test v√† th√¥ng tin cung c·∫•p",
    "roadmap": {{
    ¬† ¬† "summary": "T√≥m t·∫Øt ng·∫Øn g·ªçn l·ªô tr√¨nh 1-2 c√¢u",
    ¬† ¬† "current_status": "M·ª•c ti√™u: {prefs_dict['communication_goal']}, Th·ªùi gian: {prefs_dict['target_duration']}",
    ¬† ¬† "daily_plan_recommendation": "Khuy·∫øn ngh·ªã h·ªçc {prefs_dict['daily_commitment']} m·ªói ng√†y",
    ¬† ¬† "learning_phases": [
    ¬† ¬† ¬† ¬† {{
    ¬† ¬† ¬† ¬† ¬† ¬† "phase_name": "Giai ƒëo·∫°n 1: X√¢y d·ª±ng n·ªÅn t·∫£ng",
    ¬† ¬† ¬† ¬† ¬† ¬† "duration": "Tu·∫ßn 1-2",
    ¬† ¬† ¬† ¬† ¬† ¬† "focus_points": ["Ng·ªØ ph√°p c∆° b·∫£n", "T·ª´ v·ª±ng h√†ng ng√†y", "Ph√°t √¢m"],
    ¬† ¬† ¬† ¬† ¬† ¬† "daily_activities": [
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† {{"time_estimate": "20 ph√∫t", "activity": "H·ªçc t·ª´ v·ª±ng m·ªõi theo ch·ªß ƒë·ªÅ"}},
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† {{"time_estimate": "25 ph√∫t", "activity": "Luy·ªán c·∫•u tr√∫c c√¢u c∆° b·∫£n"}},
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† {{"time_estimate": "15 ph√∫t", "activity": "Nghe v√† nh·∫Øc l·∫°i c√¢u m·∫´u"}}
    ¬† ¬† ¬† ¬† ¬† ¬† ],
    ¬† ¬† ¬† ¬† ¬† ¬† "expected_outcomes": "N·∫Øm v·ªØng t·ª´ v·ª±ng c∆° b·∫£n v√† n√≥i ƒë∆∞·ª£c c√¢u ƒë∆°n ho√†n ch·ªânh",
    ¬† ¬† ¬† ¬† ¬† ¬† "milestone": {{
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† "milestone_name": "Ho√†n th√†nh giai ƒëo·∫°n n·ªÅn t·∫£ng",
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† "target_score_goal": "80% b√†i ki·ªÉm tra nh·ªè",
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† "milestone_requirements": [
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† "Ho√†n th√†nh 90% b√†i t·∫≠p h√†ng ng√†y",
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† "N√≥i tr√¥i ch·∫£y 10 c√¢u gi·ªõi thi·ªáu b·∫£n th√¢n"
    ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ]
    ¬† ¬† ¬† ¬† ¬† ¬† }}
    ¬† ¬† ¬† ¬† }}
    ¬† ¬† ]
    }}
    }}
    """

    try:
        roadmap_response = await run_in_threadpool(
            client.models.generate_content,
            model=GEMINI_MODEL,
            contents=[roadmap_prompt],
            config=g_types.GenerateContentConfig(response_mime_type="application/json")
        )

        roadmap_json = json.loads(roadmap_response.text)
        raw_roadmap = roadmap_json.get("roadmap", {})

        # CH·ªñ QUAN TR·ªåNG NH·∫§T ‚Äì ƒê√É S·ª¨A ƒê√öNG T√äN KEY CHO FRONTEND
        final_learning_phases = []
        for idx, phase in enumerate(raw_roadmap.get("learning_phases", [])):
            final_learning_phases.append({
                "phase_name": phase.get("phase_name") or phase.get("stage_name") or f"Giai ƒëo·∫°n {idx + 1}",
                "duration": phase.get("duration", "1-2 tu·∫ßn"),
                "focus_points": phase.get("focus_points", []),
                "daily_activities": phase.get("daily_activities", []),
                "expected_outcomes": phase.get("expected_outcomes", "C·∫£i thi·ªán k·ªπ nƒÉng c∆° b·∫£n"),
                "milestone": phase.get("milestone", {
                    "milestone_name": "Ho√†n th√†nh giai ƒëo·∫°n",
                    "target_score_goal": "80% ki·ªÉm tra",
                    "milestone_requirements": ["Ho√†n th√†nh 90% b√†i t·∫≠p"]
                })
            })

        final_roadmap = {
            "summary": raw_roadmap.get("summary", "T√≥m t·∫Øt kh√¥ng c√≥ s·∫µn do l·ªói LLM."),
            "current_status": raw_roadmap.get("current_status", f"M·ª•c ti√™u: {prefs_dict['communication_goal']}, Th·ªùi gian: {prefs_dict['target_duration']}"),
            "daily_plan_recommendation": raw_roadmap.get("daily_plan_recommendation", f"Khuy·∫øn ngh·ªã: H·ªçc {prefs_dict['daily_commitment']} m·ªói ng√†y."),
            "learning_phases": final_learning_phases,
            "diagnostic_summary": mcq_analysis,
            "speaking_transcripts": full_speaking_analysis
        }
        # --- 4. L∆ØU ROADMAP V√ÄO SUPABASE ---
        try:
            # --- 4A. Check user ƒë√£ c√≥ roadmap ch∆∞a ---
            existing = (
                admin_supabase.table("roadmaps")
                .select("*")
                .eq("user_id", payload_data.user_id)
                .maybe_single()
                .execute()
            )

            insert_data = {
                "user_id": payload_data.user_id,
                "level": mcq_analysis.get("estimated_level", "unknown"),
                "data": final_roadmap,
            }

            # --- 4B. N·∫øu ƒë√£ c√≥ ‚Üí UPDATE ---
            if existing.data:
                # N·∫øu c√≥ d·ªØ li·ªáu, maybe_single() tr·∫£ v·ªÅ dict, kh√¥ng ph·∫£i list
                roadmap_id = existing.data.get("id") 
                if not roadmap_id:
                     # Fallback n·∫øu maybe_single tr·∫£ v·ªÅ list [dict] thay v√¨ dict
                     roadmap_id = existing.data[0]["id"] if isinstance(existing.data, list) and existing.data else None
                
                if roadmap_id:
                    result = (
                        admin_supabase.table("roadmaps")
                        .update(insert_data)
                        .eq("id", roadmap_id)
                        .execute()
                    )
                    if not result.data: # Ki·ªÉm tra xem UPDATE c√≥ th·∫•t b·∫°i kh√¥ng
                        raise Exception("C·∫≠p nh·∫≠t roadmap th·∫•t b·∫°i (Kh√¥ng c√≥ d·ªØ li·ªáu tr·∫£ v·ªÅ)")
                    logger.info(f"UPDATED roadmap for user {payload_data.user_id}")
                else:
                    logger.warning("Kh√¥ng t√¨m th·∫•y ID roadmap ƒë·ªÉ c·∫≠p nh·∫≠t. Th·ª≠ INSERT m·ªõi.")
                    
                    # Th·ª≠ INSERT n·∫øu UPDATE th·∫•t b·∫°i
                    result = (
                        admin_supabase.table("roadmaps")
                        .insert(insert_data)
                        .execute()
                    )
                    if not result.data:
                         raise Exception("L∆∞u roadmap m·ªõi (fallback) th·∫•t b·∫°i")
                    logger.info(f"INSERTED new roadmap (fallback) for user {payload_data.user_id}")

            # --- 4C. N·∫øu ch∆∞a c√≥ ‚Üí INSERT ---
            else:
                result = (
                    admin_supabase.table("roadmaps")
                    .insert(insert_data)
                    .execute()
                )
                if not result.data: # Ki·ªÉm tra xem INSERT c√≥ th·∫•t b·∫°i kh√¥ng
                    raise Exception("L∆∞u roadmap m·ªõi th·∫•t b·∫°i (Kh√¥ng c√≥ d·ªØ li·ªáu tr·∫£ v·ªÅ)")
                logger.info(f"INSERTED new roadmap for user {payload_data.user_id}")

        except Exception as e:
            logger.error(f"‚ùå L·ªói l∆∞u roadmap v√†o Supabase: {e}")
            # N·∫øu l∆∞u th·∫•t b·∫°i, ta v·∫´n tr·∫£ v·ªÅ l·ªô tr√¨nh ƒë·ªÉ Frontend hi·ªÉn th·ªã t·∫°m th·ªùi
        
        return {
            "status": "success",
            "message": "Roadmap created",
            "roadmap_details": {
                "roadmap": final_roadmap,
                "diagnostic_summary": mcq_analysis
            },
            "diagnostic_summary": mcq_analysis,
            "speaking_transcripts": full_speaking_analysis
        }

    except json.JSONDecodeError as e:
        logger.error(f"JSON t·ª´ Gemini kh√¥ng h·ª£p l·ªá: {e}")
        raise HTTPException(status_code=500, detail="L·ªói ƒë·ªãnh d·∫°ng JSON t·ª´ AI")
    except Exception as e:
        logger.error(f"L·ªói t·∫°o Roadmap: {e}")
        raise HTTPException(status_code=500, detail=f"L·ªói t·∫°o l·ªô tr√¨nh: {str(e)}")
    
logger = logging.getLogger(__name__)

def get_user_roadmap(user_id: str):
    try:
        res = (
            admin_supabase.table("roadmaps")
            .select("id, user_id, level, data, created_at, updated_at")
            .eq("user_id", user_id)
            .order("created_at", desc=True)
            .limit(1)
            .execute()
        )

        # N·∫øu res.data l√† list ch·ª©a 1 dict (b√¨nh th∆∞·ªùng)
        if res.data and isinstance(res.data, list) and len(res.data) > 0:
            return res.data[0]  # tr·∫£ whole row: {'id', 'user_id', 'level', 'data', ...}
        else:
            logger.warning(f"Kh√¥ng t√¨m th·∫•y roadmap cho user: {user_id}. Supabase response: {res.data}")
            return None

    except Exception as e:
        logger.error(f"Error fetching roadmap: {e}")
        return None